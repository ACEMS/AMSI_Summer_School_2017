{
    "collab_server" : "",
    "contents" : "---\ntitle: \"A introduction to Tiday Data and Preprocessing\"\nauthor: \"Miles McBain\"\ndate: \"10/01/2017\"\noutput: html_document\n---\n\n```{r setup, include=FALSE}\nknitr::opts_chunk$set(echo = TRUE)\nlibrary(readr)\nlibrary(here)\nlibrary(knitr)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(ggplot2)\n```\n\n# Introduction \nTidy Data is the standard shape of data expected by major statistical modelling packages and frameoworks. This shape allows efficient applicaiton of split-apply-combine strategy for processing big data. This first exercise in the practical will give you hands on experience applying the principles of Tidy Data using tools from R's tidyverse (`dplyr` and `tidyr`).\n\nThe second exercise introduces feature engineering as an aspect of daa preprocessing. In the context of tidy data, feature engineering is the process of transforming columns of data to expose features of each observation to statistical modelling and machine learning algorithms. We'll use R to talk to Spark to some typical big data style processing. \n\n# Prerequisites\n* The file `prerequisites.R` contains the installation code for the R packages needed to run this prac.\n* The evironment you are running in will also need an installation of Java. See [How Do I Install Java?](https://java.com/en/download/help/download_options.xml) for more information.\n* Finally Spark 1.6.3 will need to be installed in your environment. See: [http://spark.apache.org/downloads.html](http://spark.apache.org/downloads.html)\n\n## Reading\n* [Hadley Wickham's Tidy data paper](https://www.jstatsoft.org/article/view/v059i10)\n* [Hadley Wickham's Split-Apply-Combine paper](https://www.jstatsoft.org/article/view/v040i01)\n* [Data Wrangling Cheat Sheet](https://www.rstudio.com/wp-content/uploads/2015/02/data-wrangling-cheatsheet.pdf)\n\n## 1. Tidying Data\nThis example is based on one from Hadley's [Tidy Data Vignette](ftp://cran.r-project.org/pub/R/web/packages/tidyr/vignettes/tidy-data.html). The table below is  an extract from a dataset that shows observations of Tuberculosis case instances by gender, age, country and year. The extract is of Australian cases.\n\n**Discussion*:\n\n* How do the columns map to the information?\n* What principles of Tidy data are violated?\n* What would a Tidy version of this dataset look like?\n\n### Data\n```{r, message=FALSE}\ntb_cases <- \n  read_csv(here(\"data\",\"tb_who.csv\"))\n\ntb_cases %>% \n  filter(iso2 == \"AU\") %>%\n  kable()  \n```\n\n### A Problem\nSuppose that you were interested in plotting the cases by year and gender, irrespective of country and age. By paying special attention to the structure of the data frame it may be possible to code two nested loops that can create two new columns of Male and Female aggregate results. However, processing data a row at a time is usually not performant as the data structures are optimised to be processed a column at a time. So not only will this approach fail to scale to big data, It will require a significant custom code.\n\n### A Solution\n\n#### Gather the data \nA `gather` operation, sometimes referred to as `melt`, `unpivot`, or `stack` (depending on data processing framework) is the start of the solution to this problem. You can think of a `gather` as taking a group of columns and stacking their data on top of eachother one at a time. At the same time a new key column is created that keeps track of where the data originated.  \n\nIn the example below, the new key column is `group`, while the values were stacked into `num_cases`.\n\n```{r, messges = FALSE}\ntb_cases_gathered <-\n  tb_cases %>%\n  gather(key = \"group\", value = \"num_cases\", m04:fu)\n\ntb_cases_gathered %>%\n  filter(iso2 == \"AU\", !is.na(num_cases)) %>%\n  top_n(10) %>%\n  kable()\n```\n\n#### Seprate combined columns\nThe next issue is that age and gender are now combined in the group column, since we want to ignore the effect of age this will need to be resolved. Luckily we can use `dplyr::separate()` to solve this.\n\n```{r}\ntb_cases_sep <- \n  tb_cases_gathered %>%\n  separate(col = group, into = c(\"gender\", \"age_group\"), sep = 1)\n\ntb_cases_sep %>%\n  filter(iso2 == \"AU\", !is.na(num_cases)) %>%\n  top_n(10) %>%\n  kable()\n```\n\n#### Exercise\n\n* See if you can now summarise the data such that the observations are yearly totals for each each gender. Consult the Data Wrangling Cheatsheet linked above.\n* Using the summarised data see if you can arrive at this plot using `ggplot2`:\n\n![](./figs/Tuberculosis.png)\n\n\n\n**Discuss**:\n\n* What elements of plotting did the tidy shape of the data make easier?\n* Given what you know of linear modelling how does tidy data relate to the desired from of data for fitting linear models (e.g. by least squares)?\n\n\n## 2. Feature Engineering\n\nThis example is based on historical travel time data from the M4 motorway in Sydney:\n![](./figs/m4_times.jpg)\n\nThe motorway is broken up into a series of Eastbound and Westbound \"routes\" for which we have the recorded average transit time of vehicles calculated over consecutive 3 minute intervals from March to November in 2010.\n\n### Initialisation\n```{r, message = FALSE}\n#initialise data file paths\nm4_data_file <- here(\"data\",\"m4_parquet\")\n\n\n#initialise spark\n\n#Assumes installed in your home folder. Update as necessary.\nSPARK_HOME = file.path(Sys.getenv(\"HOME\"),\"spark-1.6.3-bin-hadoop2.6\")\n\n#To install csv connecter, if required:\n#system(\"$SPARK_HOME/bin/spark-shell --packages com.databricks:spark-csv_2.10:1.5.0\")\n\nlibrary(SparkR, lib.loc = file.path(SPARK_HOME, \"R\", \"lib\"))\nsc <- sparkR.init(master = \"local[*]\", \n                  sparkEnvir = list(spark.driver.memory=\"2g\"))\nsqlContext <- sparkRSQL.init(sc)\n\n\n\n#load data to spark instance from apache file format\nm4_data <- read.parquet(sqlContext, m4_data_file)\n```\n\n### Data Exploration\nThe `route` column contains is the identifier of the stretch of road that the observed average transit time was made over. Westbound routes have IDs in 40010..400150 increasing East to West, while Eastbound routes have IDs in 41010..41160 increasing West to East.\n\n`transit` contains the average transit time in deciseconds. The average is weighted in an unspecified manner.\n\n`Date` is the time and date of the end of the 3 minute observation interval.\n\n```{r, message = FALSE}\n#show top 100 rows\nshowDF(m4_data, 10)  \n\n#count number of rows\nnrow(m4_data)\n\n#The date range for our observations\nshowDF(\n  select(m4_data, min(m4_data$Date), max(m4_data$Date))\n)\n```\n\n**Discuss**: \n\n* is this Tidy Data?\n\n### Feature Engineering and Preprocessing using Dplyr/Spark\n\n#### Preprocessing\nAn issue we have to consider is that not all routes (sections of road) are the same length. We'll standardise the transit times. \n\nStandardising using Spark:\n```{r, message = FALSE}\n#Calculate the mean and sd for each route\nsummary_tab <- \n  summarize(group_by(m4_data, m4_data$route), mean = mean(m4_data$transit), sd = sd(m4_data$transit))\nshowDF(summary_tab)\n\n#join the mean and sd back onto the main dataset\nm4_data <- join(m4_data, summary_tab, m4_data$route == summary_tab$route, \"left\")\n\n#Calculate a new column based in the Z-score\nm4_data$std_transit <- (m4_data$transit - m4_data$mean)/m4_data$sd\nshowDF(m4_data, 10)\n```\n\n**Discuss**:\n\n* How might you use `./data/RouteLengthApprox.csv` for alternate preprocessing preprocessing?\n\n\n#### Feature engineering\nThe objective of feature engineering is to transform the data to present features that will offer traction to our chosen modelling approach. A common example is processing of timestamps. Here `date`, a timestamp with granularity in seconds will probably not be useful in detecting the signal associated with transit time. Common experience would tell us that the hour of the day and day of the week may be more useful covariates.\n\n```{r, message = FALSE}\nm4_data$day_of_week <- date_format(m4_data$Date, 'EEEE')\nm4_data$hour_of_day <- hour(m4_data$Date)\nshowDF(m4_data, 10)\n```\n\n**Discuss**: \n\n* What other features might the transist time of a route be associated with?\n* Can you conceive a way to engineer them in Spark? \n\n#Shutdown\nThat's the end of the practial. When you've finished experimenting, close your Spark session with:\n```{r}\nSparkR::sparkR.stop()\n```\n\n",
    "created" : 1507272724973.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "538874732",
    "id" : "F4AC606B",
    "lastKnownWriteTime" : 1507272749,
    "last_content_update" : 1507272749631,
    "path" : "~/repos/AMSI_2017_MSBD_prac01/prac01/prac01_tidy_data.Rmd",
    "project_path" : "prac01_tidy_data.Rmd",
    "properties" : {
    },
    "relative_order" : 1,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_markdown"
}